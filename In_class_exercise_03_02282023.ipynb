{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangeethaKaveti/sangeetha_INFO5731_Spring2023/blob/main/In_class_exercise_03_02282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oIhoHXIC4UM"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1HFXd1WC4UN"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLMjDkchC4UO"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufa01cHCC4UO"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "An interesting text classification task is sentiment analysis, which involves categorizing text documents into different sentiment categories, such as positive, negative, or neutral.\n",
        "\n",
        "To build a sentiment analysis model, some useful features are:\n",
        "\n",
        "Bag-of-words: This feature represents a document as a set of its constituent words, ignoring the order in which they occur. The bag-of-words approach is useful because it captures the essence of the document without considering the order of words.\n",
        "N-grams: N-grams are a sequence of N words in a text document. In the case of sentiment analysis, unigrams (single words) and bigrams (two-word combinations) are the most commonly used. N-grams help capture the contextual meaning of words, which can be important for sentiment analysis.\n",
        "Part-of-speech (POS) tags: This feature involves labeling each word in a text document with its corresponding part of speech, such as noun, verb, adjective, or adverb. POS tags are useful for capturing the grammatical structure of a sentence, which can provide insights into the sentiment of the document.\n",
        "Named entities: Named entities are words or phrases that refer to specific people, places, or organizations. Identifying named entities can provide useful contextual information for sentiment analysis, as the sentiment associated with these entities can influence the overall sentiment of the document.\n",
        "Sentiment lexicons: Sentiment lexicons are lists of words and phrases that are associated with positive or negative sentiment. These lexicons can be used to assign sentiment scores to words in a text document, which can be aggregated to determine the overall sentiment of the document.\n",
        "By combining these features, a machine learning model can be trained to classify text documents into different sentiment categories. Bag-of-words and n-grams can be used to represent the document, while POS tags and named entities can provide additional contextual information. Finally, sentiment lexicons can be used to assign sentiment scores to words and phrases, which can be used to predict the overall sentiment of the document.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_yG82xC4UP"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFByEadpC4UP",
        "outputId": "a867b5d8-2bb3-4a4b-c65c-4b2de3dda526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words features:  [[0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 2 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1]]\n",
            "N-grams features:  [[1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1]]\n",
            "Part-of-speech (POS) features:  [['DT', 'NN', 'VBZ', 'JJ', '.', 'PRP', 'VBD', 'PRP$', 'NNS', '.', 'NN', 'MD', 'RB', 'VB', 'PRP', 'TO', 'NN', '.'], ['NN', 'VBD', 'RB', 'JJ', 'IN', 'DT', 'NN', '.', 'PRP', 'VBD', 'RB', 'VB', 'RB', 'JJ', '.'], ['RB', 'VBP', 'DT', 'NN', 'RB', 'JJ', '.', 'PRP', 'VBZ', 'VBN', 'PRP$', 'NN', 'RB', 'RB', 'JJR', '.'], ['DT', 'NN', 'VBZ', 'RB', 'RB', '.', 'PRP', 'VBD', 'RB', 'JJ', ',', 'CC', 'PRP', 'VBD', 'RB', 'JJ', 'RB', '.'], ['NN', 'MD', 'RB', 'VB', 'DT', 'NN', 'RB', '.', 'PRP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NN', '.']]\n",
            "Sentiment lexicon features:  [[0.458, 0.0, 0.542], [0.0, 0.253, 0.747], [0.408, 0.0, 0.592], [0.282, 0.127, 0.591], [0.0, 0.203, 0.797]]\n",
            "Readability features:  [[5.0, 5.466666666666667], [6.0, 5.25], [7.0, 3.642857142857143], [6.5, 4.769230769230769], [7.0, 4.142857142857143]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text\n",
        "texts = [ \"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\",  \n",
        "         \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "         \"I love this product so much! It has made my life so much easier.\",  \n",
        "         \"This product is just okay. It wasn't great, but it wasn't terrible either.\", \n",
        "         \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "# Tokenize the text\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    processed_texts.append(' '.join(tokens))\n",
        "\n",
        "# Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(processed_texts).toarray()\n",
        "\n",
        "# N-grams\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "ngram_features = ngram_vectorizer.fit_transform(processed_texts).toarray()\n",
        "\n",
        "# Part-of-speech (POS)\n",
        "pos_features = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    pos_features.append([pos for token, pos in nltk.pos_tag(tokens)])\n",
        "\n",
        "# Sentiment lexicon\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "lexicon_features = []\n",
        "for text in texts:\n",
        "    polarity_scores = sid.polarity_scores(text)\n",
        "    lexicon_features.append([polarity_scores['pos'], polarity_scores['neg'], polarity_scores['neu']])\n",
        "\n",
        "# Readability\n",
        "readability_features = []\n",
        "for text in texts:\n",
        "    words = text.split()\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(nltk.sent_tokenize(text))\n",
        "    avg_sentence_length = float(num_words/num_sentences)\n",
        "    avg_word_length = float(sum(len(word) for word in words)/num_words)\n",
        "    readability_features.append([avg_sentence_length, avg_word_length])\n",
        "\n",
        "# Print features\n",
        "print(\"Bag of Words features: \", bow_features)\n",
        "print(\"N-grams features: \", ngram_features)\n",
        "print(\"Part-of-speech (POS) features: \", pos_features)\n",
        "print(\"Sentiment lexicon features: \", lexicon_features)\n",
        "print(\"Readability features: \", readability_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V0v4TWoC4UP"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nkINwS4OWfk",
        "outputId": "75f47c07-0223-4c21-c215-fb18d0723494"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.2 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vadersentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0P_dcSxOeX1",
        "outputId": "41869055-3705-4b34-980e-cbfb0374c832"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vadersentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vadersentiment) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vadersentiment) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vadersentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vadersentiment) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->vadersentiment) (4.0.0)\n",
            "Installing collected packages: vadersentiment\n",
            "Successfully installed vadersentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "7FCNwasKC4UP",
        "outputId": "94564c10-c58a-45ae-8107-9184ceb4e5e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-278c27094466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadability_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvectorizer_ngrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'flesch_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'smog_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Chi-Square feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "import textstat\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sample data\n",
        "documents= [\"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\", \n",
        "            \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "            \"I love this product so much! It has made my life so much easier.\",    \n",
        "            \"This product is just okay. It wasn't great, but it wasn't terrible either.\",  \n",
        "            \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "# Target class labels\n",
        "labels = np.array([1, 0, 1,2,0])  # 1 for positive, 0 for negative, 2 for neutral\n",
        "\n",
        "# Bag of Words feature extraction\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_features = vectorizer_bow.fit_transform(documents)\n",
        "\n",
        "# N-Grams feature extraction\n",
        "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_features = vectorizer_ngrams.fit_transform(documents)\n",
        "\n",
        "# Part-of-speech feature extraction\n",
        "pos_vectorizer = CountVectorizer(token_pattern=r'\\b\\w\\w+\\b|!|\\?|\\\"|\\'', ngram_range=(1,1), analyzer='word', \n",
        "                                 stop_words='english')\n",
        "pos_features = pos_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Sentiment Lexicon feature extraction\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "lexicon_features = []\n",
        "for doc in documents:\n",
        "    vs = analyzer.polarity_scores(doc)\n",
        "    # apply non-negative transformation\n",
        "    lexicon_features.append([abs(vs['neg']), abs(vs['neu']), abs(vs['pos']), abs(vs['compound'])])\n",
        "lexicon_features = np.array(lexicon_features)\n",
        "\n",
        "# Readability feature extraction\n",
        "readability_features = []\n",
        "for doc in documents:\n",
        "    flesch_score = textstat.flesch_reading_ease(doc)\n",
        "    smog_score = textstat.smog_index(doc)\n",
        "    # apply non-negative transformation\n",
        "    readability_features.append([abs(flesch_score), abs(smog_score)])\n",
        "readability_features = np.array(readability_features)\n",
        "\n",
        "features = hstack((bow_features, ngram_features, pos_features, lexicon_features, readability_features))\n",
        "\n",
        "feature_names = vectorizer_bow.get_feature_names() + vectorizer_ngrams.get_feature_names() + pos_vectorizer.get_feature_names() + ['neg', 'neu', 'pos', 'compound'] + ['flesch_score', 'smog_score']\n",
        "\n",
        "# Chi-Square feature selection\n",
        "chi2_scores, _ = chi2(features, labels)\n",
        "feature_scores = list(zip(feature_names, chi2_scores))\n",
        "feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "print(\"Chi-Square scores for all features:\")\n",
        "for feature, score in feature_scores:\n",
        "    print(feature, score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-V0dFfHC4UQ"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gKnL9qCP_5y",
        "outputId": "34ca6c91-b42c-406c-a6b1-d268d5f6ea50"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=51da5e3d9234f4a855e2198064795a660d6b90e2515acf48ed6baa4ed43a4931\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.13.1 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wNOi-_gC4UQ",
        "outputId": "f31f397f-46c6-4204-c920-40372983539e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"This product is just okay. It wasn't great, but it wasn't terrible either.\", 0.6015003), ('I love this product so much! It has made my life so much easier.', 0.5684782), ('This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.', 0.4582039), (\"I was really disappointed with this product. It didn't work as advertised.\", 0.44249266), ('I would never buy this product again. It was a complete waste of money.', 0.29375678)]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Define the query and text\n",
        "query = \"I recently tried out the new restaurant in town and found the food to be decent.The service was good and the ambiance was pleasant. However, the prices were a bit on the higher side. Overall, it was a decent experience but I am not sure if I would go back again given the price point.\"\n",
        "data = [\"This product is amazing! It exceeded my expectations. I would definitely recommend it to anyone.\", \n",
        "            \"I was really disappointed with this product. It didn't work as advertised.\", \n",
        "            \"I love this product so much! It has made my life so much easier.\",    \n",
        "            \"This product is just okay. It wasn't great, but it wasn't terrible either.\",  \n",
        "            \"I would never buy this product again. It was a complete waste of money.\"]\n",
        "\n",
        "\n",
        "query_embedding = model.encode([query])[0]\n",
        "text_embeddings = model.encode(texts)\n",
        "\n",
        "similarities = cosine_similarity([query_embedding], text_embeddings)[0]\n",
        "\n",
        "ranked_texts = sorted(zip(texts, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(ranked_texts)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}